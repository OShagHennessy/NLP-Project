{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leboh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import XLMRobertaTokenizer, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import Dataset, load_dataset, DatasetDict\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 508\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load unlabelled dataset (general text data for pretraining, e.g., headlines or articles)\n",
    "unlabeled_dataset = load_dataset('text', data_files={'train': r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\health_transcripts1-xh.txt\"})  # Replace with your unlabelled data\n",
    "unlabeled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Ingaba zikhona izinto ezinokwenziwa ukuthomalalisa iimpawu ngaphandle kokusebenzisa amayeza?', 'Kubalulekile ukuzilawula izinto ezingumngcipheko kwangoko. ', 'Abantu abanesifo seswekile basemngciphekweni othe chatha wokuphathwa sisifo sentliziyo apha ebomini babo. ', 'Ingaba sisenokulungiseka?', 'Wakhe wahlaselwa sisifo sentliziyo?']}\n"
     ]
    }
   ],
   "source": [
    "print(unlabeled_dataset['train'][0:5])  # This will display the first 5 rows of the 'train' split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 406\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 51\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 51\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into train, validation, and test sets (e.g., 80% train, 10% validation, 10% test)\n",
    "unlabeled_dataset = unlabeled_dataset['train'].train_test_split(test_size=0.2)  # 80% train, 20% test\n",
    "\n",
    "# Split the test set further into validation and test (50% of 20% -> 10% validation, 10% test)\n",
    "validation_test_split = unlabeled_dataset['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "# Combine the splits into a DatasetDict\n",
    "final_unlabeled_dataset = DatasetDict({\n",
    "    'train': unlabeled_dataset['train'],\n",
    "    'validation': validation_test_split['train'],\n",
    "    'test': validation_test_split['test']\n",
    "})\n",
    "\n",
    "# Display the structure of the final split dataset\n",
    "print(final_unlabeled_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specific Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from selenium.common.exceptions import TimeoutException, NoSuchElementException, ElementClickInterceptedException\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# import pandas as pd\n",
    "# from urllib.parse import urlparse\n",
    "\n",
    "# def extract_label_from_url(url):\n",
    "#     parsed_url = urlparse(url)\n",
    "#     path = parsed_url.path\n",
    "#     label = path.split('/')[-2]  # Use the second-to-last part of the path\n",
    "#     return label\n",
    "\n",
    "# def scrape_headlines_selenium(url, num_headlines=5000):\n",
    "#     chrome_options = Options()\n",
    "#     chrome_options.add_argument(\"--headless\")\n",
    "#     service = Service(ChromeDriverManager().install())\n",
    "#     driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "#     try:\n",
    "#         driver.get(url)\n",
    "#         print(f\"Page title: {driver.title}\")\n",
    "\n",
    "#         headlines = []\n",
    "#         load_more_attempts = 0\n",
    "#         max_load_more_attempts = 10  # Adjust this value if needed\n",
    "\n",
    "#         while len(headlines) < num_headlines and load_more_attempts < max_load_more_attempts:\n",
    "#             # Wait for articles to load\n",
    "#             WebDriverWait(driver, 10).until(\n",
    "#                 EC.presence_of_element_located((By.XPATH, \"//div[contains(@class, 'td-block-span6')]\"))\n",
    "#             )\n",
    "\n",
    "#             # Find all article elements\n",
    "#             articles = driver.find_elements(By.XPATH, \"//div[contains(@class, 'td-block-span6')]\")\n",
    "\n",
    "#             # Extract headlines from new articles\n",
    "#             for article in articles[len(headlines):]:\n",
    "#                 try:\n",
    "#                     headline_element = article.find_element(By.XPATH, \".//div[contains(@class, 'td-module-thumb')]/a\")\n",
    "#                     headline = headline_element.get_attribute('title').strip()\n",
    "#                     if headline not in headlines:\n",
    "#                         headlines.append(headline)\n",
    "#                         # print(f\"Found headline: {headline}\")\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error extracting headline: {e}\")\n",
    "\n",
    "#             print(f\"Total headlines found: {len(headlines)}\")\n",
    "\n",
    "#             if len(headlines) >= num_headlines:\n",
    "#                 break\n",
    "\n",
    "#             # Try to click 'Load more' button\n",
    "#             load_more_clicked = False\n",
    "#             try:\n",
    "#                 # Scroll to the bottom of the page\n",
    "#                 driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "#                 time.sleep(2)  # Wait for any lazy-loaded content\n",
    "\n",
    "#                 # Try to find the 'Load more' button\n",
    "#                 load_more_button = WebDriverWait(driver, 5).until(\n",
    "#                     EC.element_to_be_clickable((By.ID, \"load_more_articles_button\"))\n",
    "#                 )\n",
    "#                 driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", load_more_button)\n",
    "#                 time.sleep(1)  # Short pause after scrolling\n",
    "#                 load_more_button.click()\n",
    "#                 print(\"Clicked 'Load more' button\")\n",
    "#                 load_more_clicked = True\n",
    "#                 time.sleep(3)  # Wait for new content to load\n",
    "#             except (TimeoutException, NoSuchElementException, ElementClickInterceptedException) as e:\n",
    "#                 print(f\"Could not click 'Load more' button: {e}\")\n",
    "                \n",
    "#                 # Try alternative method using JavaScript click\n",
    "#                 try:\n",
    "#                     driver.execute_script(\"document.getElementById('load_more_articles_button').click();\")\n",
    "#                     print(\"Clicked 'Load more' button using JavaScript\")\n",
    "#                     load_more_clicked = True\n",
    "#                     time.sleep(3)  # Wait for new content to load\n",
    "#                 except Exception as js_e:\n",
    "#                     print(f\"Could not click 'Load more' button using JavaScript: {js_e}\")\n",
    "\n",
    "#             if not load_more_clicked:\n",
    "#                 load_more_attempts += 1\n",
    "#                 print(f\"Load more attempt {load_more_attempts} failed\")\n",
    "#             else:\n",
    "#                 load_more_attempts = 0  # Reset the counter if we successfully loaded more\n",
    "\n",
    "#         if load_more_attempts >= max_load_more_attempts:\n",
    "#             print(\"Reached maximum 'Load more' attempts. Ending search.\")\n",
    "\n",
    "#         return headlines[:num_headlines]\n",
    "\n",
    "#     finally:\n",
    "#         driver.quit()\n",
    "\n",
    "# # URL of the webpage\n",
    "# url = 'https://www.isolezwelesixhosa.co.za/ezemidlalo/'\n",
    "\n",
    "# print(\"Attempting to scrape with Selenium:\")\n",
    "# headlines = scrape_headlines_selenium(url)\n",
    "\n",
    "# # Extract label from URL\n",
    "# label = extract_label_from_url(url)\n",
    "\n",
    "# # Create DataFrame with headlines and label\n",
    "# df = pd.DataFrame({\n",
    "#     'headlines': headlines,\n",
    "#     'label': [label] * len(headlines)\n",
    "# })\n",
    "\n",
    "# # Save the headlines to a CSV file\n",
    "# df.to_csv('headlines.csv', index=False)\n",
    "\n",
    "# print(f\"\\n{len(headlines)} Headlines saved to headlines.csv\")\n",
    "# print(f\"Label used: {label}\")\n",
    "\n",
    "# # Print the first few headlines for verification\n",
    "# print(\"\\nFirst few headlines:\")\n",
    "# for headline in headlines[:5]:\n",
    "#     print(f\"{headline} (Label: {label})\")\n",
    "\n",
    "# # Print the last few headlines for verification\n",
    "# print(\"\\nLast few headlines:\")\n",
    "# for headline in headlines[-5:]:\n",
    "#     print(f\"{headline} (Label: {label})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # URL of the webpage\n",
    "# url = 'https://www.isolezwelesixhosa.co.za/iindaba/'\n",
    "\n",
    "# print(\"Attempting to scrape with Selenium:\")\n",
    "# headlines = scrape_headlines_selenium(url)\n",
    "\n",
    "# # Extract label from URL\n",
    "# label = extract_label_from_url(url)\n",
    "\n",
    "# # Create DataFrame with headlines and label\n",
    "# df = pd.DataFrame({\n",
    "#     'headlines': headlines,\n",
    "#     'label': [label] * len(headlines)\n",
    "# })\n",
    "\n",
    "# # Save the headlines to a CSV file\n",
    "# df.to_csv('more_headlines.csv', index=False)\n",
    "\n",
    "# print(f\"\\n{len(headlines)} Headlines saved to more_headlines.csv\")\n",
    "# print(f\"Label used: {label}\")\n",
    "\n",
    "# # Print the first few headlines for verification\n",
    "# print(\"\\nFirst few headlines:\")\n",
    "# for headline in headlines[:5]:\n",
    "#     print(f\"{headline} (Label: {label})\")\n",
    "\n",
    "# # Print the last few headlines for verification\n",
    "# print(\"\\nLast few headlines:\")\n",
    "# for headline in headlines[-5:]:\n",
    "#     print(f\"{headline} (Label: {label})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1 = pd.read_csv(\"headlines.csv\")\n",
    "# data2 = pd.read_csv(\"more_headlines.csv\")\n",
    "\n",
    "# df = pd.concat([data1, data2], ignore_index=True)\n",
    "# df.dropna(inplace=True)\n",
    "# df.to_csv(\"xhosa headlines dataset.csv\")\n",
    "\n",
    "\n",
    "# import re\n",
    "# import pandas as pd\n",
    "\n",
    "# # Function to remove punctuation from a single string\n",
    "# def remove_punctuation(text):\n",
    "#     # Check if the input is a string, otherwise return it unchanged\n",
    "#     if isinstance(text, str):\n",
    "#         pattern = r'[^\\w\\s]'  # Regex pattern to match any punctuation\n",
    "#         return re.sub(pattern, '', text)\n",
    "\n",
    "\n",
    "# # Apply the remove_punctuation function to the 'headlines' column\n",
    "# df['headlines'] = df['headlines'].apply(remove_punctuation)\n",
    "\n",
    "# # Display the updated DataFrame\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Step 1: Split 85% for train + test and 15% for validation\n",
    "# train_test_df, val_df = train_test_split(df, test_size=0.15, random_state=42)\n",
    "\n",
    "# # Step 2: Split train_test_df into 70% training and 15% testing (equivalent to 85% * 70/85 for training)\n",
    "# train_df, test_df = train_test_split(train_test_df, test_size=0.1765, random_state=42)  # 0.1765 ≈ 15 / 85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.to_csv(\"train_data.csv\", index=False)\n",
    "# test_df.to_csv(\"test_data.csv\", index=False)\n",
    "# val_df.to_csv(\"val_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# # Function to remove punctuation from a single string\n",
    "# def remove_punctuation(text):\n",
    "#     # Check if the input is a string, otherwise return it unchanged\n",
    "#     if isinstance(text, str):\n",
    "#         pattern = r'[^\\w\\s]'  # Regex pattern to match any punctuation\n",
    "#         return re.sub(pattern, '', text)\n",
    "\n",
    "# # Function to remove punctuation from a text file\n",
    "# def remove_punctuation_from_file(file_path):\n",
    "#     try:\n",
    "#         # Open the file with UTF-8 encoding and read its contents\n",
    "#         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#             text = file.read()\n",
    "\n",
    "#         # Remove punctuation from the entire text\n",
    "#         clean_text = remove_punctuation(text)\n",
    "\n",
    "#         # Save the clean text back to a file (you can overwrite the original file or save to a new file)\n",
    "#         with open('cleaned_' + file_path, 'w', encoding='utf-8') as clean_file:\n",
    "#             clean_file.write(clean_text)\n",
    "#     except UnicodeDecodeError:\n",
    "#         print(f\"Error: Unable to read the file {file_path} due to encoding issues.\")\n",
    "\n",
    "# # Example usage\n",
    "# file_path = 'xhosa constitution.txt'  # Replace with your text file path\n",
    "# remove_punctuation_from_file(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Check GPU availability\n",
    "print(torch.cuda.is_available())  # Should return True if the GPU is accessible\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Use the GPU if available\n",
    "    print(torch.cuda.get_device_name(0))  # Should print the GPU name\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Use CPU if no GPU is available\n",
    "\n",
    "model_name = 'FacebookAI/xlm-roberta-base'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Task Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leboh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Map: 100%|██████████| 406/406 [00:00<00:00, 4060.38 examples/s]\n",
      "Map: 100%|██████████| 51/51 [00:00<00:00, 1728.06 examples/s]\n",
      "Map: 100%|██████████| 51/51 [00:00<00:00, 1545.67 examples/s]\n",
      "100%|██████████| 250/250 [3:09:36<00:00, 45.51s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 11376.8888, 'train_samples_per_second': 0.178, 'train_steps_per_second': 0.022, 'train_loss': 3.834228759765625, 'epoch': 4.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=3.834228759765625, metrics={'train_runtime': 11376.8888, 'train_samples_per_second': 0.178, 'train_steps_per_second': 0.022, 'total_flos': 527758110720000.0, 'train_loss': 3.834228759765625, 'epoch': 4.926108374384237})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ======== Step 1: Pretraining on Unlabelled Data (Masked Language Model - MLM) ========\n",
    "\n",
    "# Load the XLM-RoBERTa tokenizer and model for MLM\n",
    "model_name = \"xlm-roberta-base\"  # Specify your model name here\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model_mlm = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_mlm = model_mlm.to(device)\n",
    "# Load unlabelled dataset (general text data for pretraining, e.g., headlines or articles)\n",
    "# unlabeled_dataset = load_dataset('text', data_files={'train': r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\health_transcripts1-xh.txt\"})  # Replace with your unlabelled data\n",
    "\n",
    "# Tokenize the unlabelled dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], return_special_tokens_mask=True, truncation=True, padding='max_length')\n",
    "\n",
    "tokenized_unlabeled_dataset = final_unlabeled_dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n",
    "\n",
    "# Custom function to compute accuracy and perplexity\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Masked tokens have label -100, ignore them in accuracy computation\n",
    "    mask = labels != -100\n",
    "    correct_predictions = (predictions == labels) & mask\n",
    "\n",
    "    accuracy = correct_predictions.sum() / mask.sum()\n",
    "    \n",
    "    # Perplexity calculation\n",
    "    loss = eval_pred.loss\n",
    "    perplexity = np.exp(loss)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'perplexity': perplexity\n",
    "    }\n",
    "\n",
    "# Create a data collator for MLM\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15  # Mask 15% of tokens\n",
    ")\n",
    "\n",
    "# Pretraining arguments\n",
    "pretraining_args = TrainingArguments(\n",
    "    output_dir='./pretrained_xlm_roberta',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=2,  # Reduced batch size\n",
    "    gradient_accumulation_steps=4,  # Gradient accumulation\n",
    "    fp16=True,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Initialize Trainer for MLM pretraining\n",
    "trainer_mlm = Trainer(\n",
    "    model=model_mlm,\n",
    "    args=pretraining_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_unlabeled_dataset['train'],\n",
    "    eval_dataset=tokenized_unlabeled_dataset['validation'],  # Use validation set for evaluation\n",
    "    compute_metrics=compute_metrics\n",
    "\n",
    ")\n",
    "\n",
    "# Pretrain the model\n",
    "trainer_mlm.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./pretrained_xlm_roberta\\\\tokenizer_config.json',\n",
       " './pretrained_xlm_roberta\\\\special_tokens_map.json',\n",
       " './pretrained_xlm_roberta\\\\sentencepiece.bpe.model',\n",
       " './pretrained_xlm_roberta\\\\added_tokens.json',\n",
       " './pretrained_xlm_roberta\\\\tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the pretrained MLM model\n",
    "model_mlm.save_pretrained('./pretrained_xlm_roberta')\n",
    "tokenizer.save_pretrained('./pretrained_xlm_roberta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\leboh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from seaborn) (2.1.0)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\leboh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\leboh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from seaborn) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\leboh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\leboh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\leboh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\leboh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\leboh\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\leboh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\leboh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\leboh\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\leboh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\leboh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\leboh\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specific Task Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the pretrained tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('./pretrained_xlm_roberta')\n",
    "model_classifier = AutoModelForSequenceClassification.from_pretrained('./pretrained_xlm_roberta', num_labels=2)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('csv', data_files={'train': 'train_data.csv', 'val': 'val_data.csv'})\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(['iindaba', 'ezemidlalo'])  # Adjust these labels if needed\n",
    "\n",
    "def encode_labels(examples):\n",
    "    examples['labels'] = label_encoder.transform(examples['label'])\n",
    "    return examples\n",
    "\n",
    "# Apply label encoding to the dataset\n",
    "dataset = dataset.map(encode_labels, batched=True)\n",
    "\n",
    "def tokenize_classification(examples):\n",
    "    # Ensure 'headlines' is a list of strings\n",
    "    headlines = examples['headlines']\n",
    "    if isinstance(headlines, str):\n",
    "        headlines = [headlines]\n",
    "    \n",
    "    # Tokenize the headlines\n",
    "    tokenized = tokenizer(\n",
    "        headlines,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512  # Adjust this value if needed\n",
    "    )\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# Apply the tokenizer function to the dataset\n",
    "tokenized_labeled_dataset = dataset.map(\n",
    "    tokenize_classification,\n",
    "    batched=True,\n",
    "    remove_columns=['headlines', 'label']\n",
    ")\n",
    "\n",
    "# Fine-tuning arguments\n",
    "fine_tuning_args = TrainingArguments(\n",
    "    output_dir='./finetuned_xlm_roberta',\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,  # Gradient accumulation\n",
    "    fp16=True,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Initialize Trainer for fine-tuning\n",
    "trainer_classifier = Trainer(\n",
    "    model=model_classifier,\n",
    "    args=fine_tuning_args,\n",
    "    train_dataset=tokenized_labeled_dataset['train'],\n",
    "    eval_dataset=tokenized_labeled_dataset['val'],\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer_classifier.train()\n",
    "\n",
    "# Save the fine-tuned classifier model\n",
    "model_classifier.save_pretrained('./finetuned_xlm_roberta')\n",
    "tokenizer.save_pretrained('./finetuned_xlm_roberta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('./finetuned_xlm_roberta')\n",
    "model_classifier = AutoModelForSequenceClassification.from_pretrained('./finetuned_xlm_roberta')\n",
    "\n",
    "# Load the validation dataset\n",
    "dataset = load_dataset('csv', data_files={'val': 'val_data.csv'})\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(['iindaba', 'ezemidlalo'])  # Adjust these labels if needed\n",
    "\n",
    "# Encode the labels correctly\n",
    "def encode_labels(examples):\n",
    "    examples['labels'] = label_encoder.transform(examples['label'])  # Ensure 'label' is transformed into numeric form\n",
    "    return examples\n",
    "\n",
    "# Apply encoding to the dataset\n",
    "dataset = dataset.map(encode_labels, batched=True)\n",
    "\n",
    "# Tokenize the input dataset\n",
    "def tokenize_classification(examples):\n",
    "    return tokenizer(\n",
    "        examples['headlines'],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt'  # Return PyTorch tensors\n",
    "    )\n",
    "\n",
    "# Apply tokenization to the validation set\n",
    "tokenized_dataset = dataset['val'].map(tokenize_classification, batched=True)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Define a DataLoader for batching\n",
    "batch_size = 16  # Adjust this based on your memory capacity\n",
    "dataloader = DataLoader(tokenized_dataset, batch_size=batch_size)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_classifier.to(device)\n",
    "\n",
    "# Function to extract hidden states from the model\n",
    "def extract_hidden_states(model, dataloader):\n",
    "    model.eval()  # Set the model in evaluation mode\n",
    "    hidden_states = []\n",
    "    tokens = []  # Store the tokens (subwords)\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            # Get the model's output with hidden states\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "            \n",
    "            # Extract the last hidden layer\n",
    "            last_hidden_state = outputs.hidden_states[-1]\n",
    "            \n",
    "            # Mean pooling across the sequence length (dim=1)\n",
    "            pooled_hidden_state = last_hidden_state.mean(dim=1)\n",
    "            \n",
    "            hidden_states.append(pooled_hidden_state.cpu().numpy())\n",
    "            \n",
    "            # Extract tokens for each input\n",
    "            for batch_input_ids in input_ids:\n",
    "                tokens.append([tokenizer.decode(token_id) for token_id in batch_input_ids if token_id not in tokenizer.all_special_ids])\n",
    "\n",
    "    return np.vstack(hidden_states), tokens  # Return hidden states and tokens\n",
    "\n",
    "# Extract hidden states and tokens for the validation set\n",
    "hidden_states, tokens = extract_hidden_states(model_classifier, dataloader)\n",
    "\n",
    "# Apply t-SNE to reduce dimensionality to 2 components\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)  # Adjust perplexity and n_iter if needed\n",
    "tsne_result = tsne.fit_transform(hidden_states)\n",
    "\n",
    "# Extract true labels from the validation dataset (make sure labels are numeric)\n",
    "true_labels_tensor = torch.tensor(tokenized_dataset['labels'])\n",
    "true_labels = true_labels_tensor.numpy()\n",
    "\n",
    "# Prepare the token data for the plot\n",
    "# Flatten the list of tokens so that each token corresponds to a t-SNE point\n",
    "flat_tokens = [token for token_list in tokens for token in token_list]\n",
    "flat_tsne_result = np.repeat(tsne_result, [len(token_list) for token_list in tokens], axis=0)\n",
    "flat_labels = np.repeat(true_labels, [len(token_list) for token_list in tokens])\n",
    "\n",
    "# Create a DataFrame with t-SNE results and token information\n",
    "df_tsne = pd.DataFrame(flat_tsne_result, columns=['TSNE Component 1', 'TSNE Component 2'])\n",
    "df_tsne['Label'] = flat_labels\n",
    "df_tsne['Token'] = flat_tokens\n",
    "\n",
    "# Create an interactive t-SNE plot using Plotly\n",
    "fig = px.scatter(df_tsne, x='TSNE Component 1', y='TSNE Component 2',\n",
    "                 color='Label',\n",
    "                 hover_name='Token',  # Show token as hover information\n",
    "                 title=\"t-SNE of Hidden States from XLM-RoBERTa (Tokens)\",\n",
    "                 labels={'Label': 'Class'},\n",
    "                 color_continuous_scale='Viridis')\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Predictions and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the pretrained tokenizer and fine-tuned model\n",
    "tokenizer = AutoTokenizer.from_pretrained('./finetuned_xlm_roberta')\n",
    "model_classifier = AutoModelForSequenceClassification.from_pretrained('./finetuned_xlm_roberta')\n",
    "\n",
    "# Load the test dataset\n",
    "test_dataset = load_dataset('csv', data_files={'test': 'test_data.csv'})['test']\n",
    "\n",
    "# Encode the labels using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(['iindaba', 'ezemidlalo'])  # Modify labels according to your dataset\n",
    "\n",
    "def encode_labels(examples):\n",
    "    examples['labels'] = label_encoder.transform(examples['label'])\n",
    "    return examples\n",
    "\n",
    "# Apply label encoding to the test dataset\n",
    "test_dataset = test_dataset.map(encode_labels, batched=True)\n",
    "\n",
    "# Tokenize the test dataset\n",
    "def tokenize_classification(examples):\n",
    "    return tokenizer(\n",
    "        examples['headlines'],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(\n",
    "    tokenize_classification,\n",
    "    batched=True,\n",
    "    remove_columns=['headlines', 'label']  # Remove original columns\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer_classifier = Trainer(\n",
    "    model=model_classifier,\n",
    ")\n",
    "\n",
    "# Make predictions on the tokenized test dataset\n",
    "predictions = trainer_classifier.predict(tokenized_test_dataset)\n",
    "\n",
    "# Extract logits and predicted class labels\n",
    "logits = predictions.predictions\n",
    "predicted_label_ids = np.argmax(logits, axis=-1)\n",
    "\n",
    "# True labels (after tokenization)\n",
    "true_labels = tokenized_test_dataset['labels']\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_label_ids)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('FacebookAI/xlm-roberta-base')\n",
    "base_model_classifier = AutoModelForSequenceClassification.from_pretrained('FacebookAI/xlm-roberta-base')\n",
    "\n",
    "# Load the test dataset\n",
    "test_dataset = load_dataset('csv', data_files={'test': 'test_data.csv'})['test']\n",
    "\n",
    "# Encode the labels using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(['iindaba', 'ezemidlalo'])  # Modify labels according to your dataset\n",
    "\n",
    "def encode_labels(examples):\n",
    "    examples['labels'] = label_encoder.transform(examples['label'])\n",
    "    return examples\n",
    "\n",
    "# Apply label encoding to the test dataset\n",
    "test_dataset = test_dataset.map(encode_labels, batched=True)\n",
    "\n",
    "# Tokenize the test dataset\n",
    "def tokenize_classification(examples):\n",
    "    return tokenizer(\n",
    "        examples['headlines'],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(\n",
    "    tokenize_classification,\n",
    "    batched=True,\n",
    "    remove_columns=['headlines', 'label']  # Remove original columns\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer_classifier = Trainer(\n",
    "    model=base_model_classifier,\n",
    ")\n",
    "\n",
    "# Make predictions on the tokenized test dataset\n",
    "predictions = trainer_classifier.predict(tokenized_test_dataset)\n",
    "\n",
    "# Extract logits and predicted class labels\n",
    "logits = predictions.predictions\n",
    "predicted_label_ids = np.argmax(logits, axis=-1)\n",
    "\n",
    "# True labels (after tokenization)\n",
    "true_labels = tokenized_test_dataset['labels']\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_label_ids)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
