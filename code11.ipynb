{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import XLMRobertaTokenizer, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import Dataset, load_dataset, DatasetDict\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenation complete. Saved to C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\concatenated_transcripts.txt\n"
     ]
    }
   ],
   "source": [
    "# Example: List of files to concatenate\n",
    "files_to_concatenate = [\n",
    "    r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\health_transcripts1-xh.txt\",\n",
    "    r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\health_transcripts2-xh.txt\",\n",
    "    r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\health_transcripts3-xh.txt\",\n",
    "    r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\mobile_xhosa_mono-xh.txt\",\n",
    "    r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_aa-xh.txt\",\n",
    "    r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_ab-xh.txt\",\n",
    "    r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_ac-xh.txt\",\n",
    "    r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_ad-xh.txt\",\n",
    "    r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_ae-xh.txt\",\n",
    "    r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_af-xh.txt\",\n",
    "    r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_ag-xh.txt\",\n",
    "    r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_ah-xh.txt\",\n",
    "    r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_ai-xh.txt\",\n",
    "    r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_aj-xh.txt\",\n",
    "    r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_ak-xh.txt\",\n",
    "    r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_al-xh.txt\",\n",
    "    r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_am-xh.txt\",\n",
    "    r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_an-xh.txt\",\n",
    "    r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_ao-xh.txt\",\n",
    "    r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_ap-xh.txt\",\n",
    "    r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_aq-xh.txt\",\n",
    "    r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_ar-xh.txt\",\n",
    "    r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_as-xh.txt\",\n",
    "    r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_at-xh.txt\",\n",
    "    r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_au-xh.txt\",\n",
    "    r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_av-xh.txt\",\n",
    "    r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_aw-xh.txt\",\n",
    "    r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_ax-xh.txt\",\n",
    "    r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\westerncape-health-xh.txt\",\n",
    "]\n",
    "\n",
    "# Initialize an empty string to store the concatenated content\n",
    "concatenated_content = \"\"\n",
    "\n",
    "# Loop through each file and concatenate their content\n",
    "for file_path in files_to_concatenate:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        concatenated_content += file.read() + \"\\n\"  # Add a newline between files if desired\n",
    "\n",
    "# Save the concatenated content to a new file\n",
    "output_file = r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\concatenated_transcripts.txt\"\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    file.write(concatenated_content)\n",
    "\n",
    "print(f\"Concatenation complete. Saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 11005 examples [00:00, 343174.30 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 11005\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load unlabelled dataset (general text data for pretraining, e.g., headlines or articles)\n",
    "unlabeled_dataset = load_dataset('text', data_files={'train': r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\concatenated_transcripts.txt\"})  # Replace with your unlabelled data\n",
    "unlabeled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Ingaba zikhona izinto ezinokwenziwa ukuthomalalisa iimpawu ngaphandle kokusebenzisa amayeza?', 'Kubalulekile ukuzilawula izinto ezingumngcipheko kwangoko. ', 'Abantu abanesifo seswekile basemngciphekweni othe chatha wokuphathwa sisifo sentliziyo apha ebomini babo. ', 'Ingaba sisenokulungiseka?', 'Wakhe wahlaselwa sisifo sentliziyo?']}\n"
     ]
    }
   ],
   "source": [
    "print(unlabeled_dataset['train'][0:5])  # This will display the first 5 rows of the 'train' split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 8804\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1100\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1101\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into train, validation, and test sets (e.g., 80% train, 10% validation, 10% test)\n",
    "unlabeled_dataset = unlabeled_dataset['train'].train_test_split(test_size=0.2)  # 80% train, 20% test\n",
    "\n",
    "# Split the test set further into validation and test (50% of 20% -> 10% validation, 10% test)\n",
    "validation_test_split = unlabeled_dataset['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "# Combine the splits into a DatasetDict\n",
    "final_unlabeled_dataset = DatasetDict({\n",
    "    'train': unlabeled_dataset['train'],\n",
    "    'validation': validation_test_split['train'],\n",
    "    'test': validation_test_split['test']\n",
    "})\n",
    "\n",
    "# Display the structure of the final split dataset\n",
    "print(final_unlabeled_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specific Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from selenium.common.exceptions import TimeoutException, NoSuchElementException, ElementClickInterceptedException\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# import pandas as pd\n",
    "# from urllib.parse import urlparse\n",
    "\n",
    "# def extract_label_from_url(url):\n",
    "#     parsed_url = urlparse(url)\n",
    "#     path = parsed_url.path\n",
    "#     label = path.split('/')[-2]  # Use the second-to-last part of the path\n",
    "#     return label\n",
    "\n",
    "# def scrape_headlines_selenium(url, num_headlines=5000):\n",
    "#     chrome_options = Options()\n",
    "#     chrome_options.add_argument(\"--headless\")\n",
    "#     service = Service(ChromeDriverManager().install())\n",
    "#     driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "#     try:\n",
    "#         driver.get(url)\n",
    "#         print(f\"Page title: {driver.title}\")\n",
    "\n",
    "#         headlines = []\n",
    "#         load_more_attempts = 0\n",
    "#         max_load_more_attempts = 10  # Adjust this value if needed\n",
    "\n",
    "#         while len(headlines) < num_headlines and load_more_attempts < max_load_more_attempts:\n",
    "#             # Wait for articles to load\n",
    "#             WebDriverWait(driver, 10).until(\n",
    "#                 EC.presence_of_element_located((By.XPATH, \"//div[contains(@class, 'td-block-span6')]\"))\n",
    "#             )\n",
    "\n",
    "#             # Find all article elements\n",
    "#             articles = driver.find_elements(By.XPATH, \"//div[contains(@class, 'td-block-span6')]\")\n",
    "\n",
    "#             # Extract headlines from new articles\n",
    "#             for article in articles[len(headlines):]:\n",
    "#                 try:\n",
    "#                     headline_element = article.find_element(By.XPATH, \".//div[contains(@class, 'td-module-thumb')]/a\")\n",
    "#                     headline = headline_element.get_attribute('title').strip()\n",
    "#                     if headline not in headlines:\n",
    "#                         headlines.append(headline)\n",
    "#                         # print(f\"Found headline: {headline}\")\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error extracting headline: {e}\")\n",
    "\n",
    "#             print(f\"Total headlines found: {len(headlines)}\")\n",
    "\n",
    "#             if len(headlines) >= num_headlines:\n",
    "#                 break\n",
    "\n",
    "#             # Try to click 'Load more' button\n",
    "#             load_more_clicked = False\n",
    "#             try:\n",
    "#                 # Scroll to the bottom of the page\n",
    "#                 driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "#                 time.sleep(2)  # Wait for any lazy-loaded content\n",
    "\n",
    "#                 # Try to find the 'Load more' button\n",
    "#                 load_more_button = WebDriverWait(driver, 5).until(\n",
    "#                     EC.element_to_be_clickable((By.ID, \"load_more_articles_button\"))\n",
    "#                 )\n",
    "#                 driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", load_more_button)\n",
    "#                 time.sleep(1)  # Short pause after scrolling\n",
    "#                 load_more_button.click()\n",
    "#                 print(\"Clicked 'Load more' button\")\n",
    "#                 load_more_clicked = True\n",
    "#                 time.sleep(3)  # Wait for new content to load\n",
    "#             except (TimeoutException, NoSuchElementException, ElementClickInterceptedException) as e:\n",
    "#                 print(f\"Could not click 'Load more' button: {e}\")\n",
    "                \n",
    "#                 # Try alternative method using JavaScript click\n",
    "#                 try:\n",
    "#                     driver.execute_script(\"document.getElementById('load_more_articles_button').click();\")\n",
    "#                     print(\"Clicked 'Load more' button using JavaScript\")\n",
    "#                     load_more_clicked = True\n",
    "#                     time.sleep(3)  # Wait for new content to load\n",
    "#                 except Exception as js_e:\n",
    "#                     print(f\"Could not click 'Load more' button using JavaScript: {js_e}\")\n",
    "\n",
    "#             if not load_more_clicked:\n",
    "#                 load_more_attempts += 1\n",
    "#                 print(f\"Load more attempt {load_more_attempts} failed\")\n",
    "#             else:\n",
    "#                 load_more_attempts = 0  # Reset the counter if we successfully loaded more\n",
    "\n",
    "#         if load_more_attempts >= max_load_more_attempts:\n",
    "#             print(\"Reached maximum 'Load more' attempts. Ending search.\")\n",
    "\n",
    "#         return headlines[:num_headlines]\n",
    "\n",
    "#     finally:\n",
    "#         driver.quit()\n",
    "\n",
    "# # URL of the webpage\n",
    "# url = 'https://www.isolezwelesixhosa.co.za/ezemidlalo/'\n",
    "\n",
    "# print(\"Attempting to scrape with Selenium:\")\n",
    "# headlines = scrape_headlines_selenium(url)\n",
    "\n",
    "# # Extract label from URL\n",
    "# label = extract_label_from_url(url)\n",
    "\n",
    "# # Create DataFrame with headlines and label\n",
    "# df = pd.DataFrame({\n",
    "#     'headlines': headlines,\n",
    "#     'label': [label] * len(headlines)\n",
    "# })\n",
    "\n",
    "# # Save the headlines to a CSV file\n",
    "# df.to_csv('headlines.csv', index=False)\n",
    "\n",
    "# print(f\"\\n{len(headlines)} Headlines saved to headlines.csv\")\n",
    "# print(f\"Label used: {label}\")\n",
    "\n",
    "# # Print the first few headlines for verification\n",
    "# print(\"\\nFirst few headlines:\")\n",
    "# for headline in headlines[:5]:\n",
    "#     print(f\"{headline} (Label: {label})\")\n",
    "\n",
    "# # Print the last few headlines for verification\n",
    "# print(\"\\nLast few headlines:\")\n",
    "# for headline in headlines[-5:]:\n",
    "#     print(f\"{headline} (Label: {label})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # URL of the webpage\n",
    "# url = 'https://www.isolezwelesixhosa.co.za/iindaba/'\n",
    "\n",
    "# print(\"Attempting to scrape with Selenium:\")\n",
    "# headlines = scrape_headlines_selenium(url)\n",
    "\n",
    "# # Extract label from URL\n",
    "# label = extract_label_from_url(url)\n",
    "\n",
    "# # Create DataFrame with headlines and label\n",
    "# df = pd.DataFrame({\n",
    "#     'headlines': headlines,\n",
    "#     'label': [label] * len(headlines)\n",
    "# })\n",
    "\n",
    "# # Save the headlines to a CSV file\n",
    "# df.to_csv('more_headlines.csv', index=False)\n",
    "\n",
    "# print(f\"\\n{len(headlines)} Headlines saved to more_headlines.csv\")\n",
    "# print(f\"Label used: {label}\")\n",
    "\n",
    "# # Print the first few headlines for verification\n",
    "# print(\"\\nFirst few headlines:\")\n",
    "# for headline in headlines[:5]:\n",
    "#     print(f\"{headline} (Label: {label})\")\n",
    "\n",
    "# # Print the last few headlines for verification\n",
    "# print(\"\\nLast few headlines:\")\n",
    "# for headline in headlines[-5:]:\n",
    "#     print(f\"{headline} (Label: {label})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1 = pd.read_csv(\"headlines.csv\")\n",
    "# data2 = pd.read_csv(\"more_headlines.csv\")\n",
    "\n",
    "# df = pd.concat([data1, data2], ignore_index=True)\n",
    "# df.dropna(inplace=True)\n",
    "# df.to_csv(\"xhosa headlines dataset.csv\")\n",
    "\n",
    "\n",
    "# import re\n",
    "# import pandas as pd\n",
    "\n",
    "# # Function to remove punctuation from a single string\n",
    "# def remove_punctuation(text):\n",
    "#     # Check if the input is a string, otherwise return it unchanged\n",
    "#     if isinstance(text, str):\n",
    "#         pattern = r'[^\\w\\s]'  # Regex pattern to match any punctuation\n",
    "#         return re.sub(pattern, '', text)\n",
    "\n",
    "\n",
    "# # Apply the remove_punctuation function to the 'headlines' column\n",
    "# df['headlines'] = df['headlines'].apply(remove_punctuation)\n",
    "\n",
    "# # Display the updated DataFrame\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Step 1: Split 85% for train + test and 15% for validation\n",
    "# train_test_df, val_df = train_test_split(df, test_size=0.15, random_state=42)\n",
    "\n",
    "# # Step 2: Split train_test_df into 70% training and 15% testing (equivalent to 85% * 70/85 for training)\n",
    "# train_df, test_df = train_test_split(train_test_df, test_size=0.1765, random_state=42)  # 0.1765 ≈ 15 / 85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.to_csv(\"train_data.csv\", index=False)\n",
    "# test_df.to_csv(\"test_data.csv\", index=False)\n",
    "# val_df.to_csv(\"val_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# # Function to remove punctuation from a single string\n",
    "# def remove_punctuation(text):\n",
    "#     # Check if the input is a string, otherwise return it unchanged\n",
    "#     if isinstance(text, str):\n",
    "#         pattern = r'[^\\w\\s]'  # Regex pattern to match any punctuation\n",
    "#         return re.sub(pattern, '', text)\n",
    "\n",
    "# # Function to remove punctuation from a text file\n",
    "# def remove_punctuation_from_file(file_path):\n",
    "#     try:\n",
    "#         # Open the file with UTF-8 encoding and read its contents\n",
    "#         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#             text = file.read()\n",
    "\n",
    "#         # Remove punctuation from the entire text\n",
    "#         clean_text = remove_punctuation(text)\n",
    "\n",
    "#         # Save the clean text back to a file (you can overwrite the original file or save to a new file)\n",
    "#         with open('cleaned_' + file_path, 'w', encoding='utf-8') as clean_file:\n",
    "#             clean_file.write(clean_text)\n",
    "#     except UnicodeDecodeError:\n",
    "#         print(f\"Error: Unable to read the file {file_path} due to encoding issues.\")\n",
    "\n",
    "# # Example usage\n",
    "# file_path = 'xhosa constitution.txt'  # Replace with your text file path\n",
    "# remove_punctuation_from_file(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# # Step 1: Check GPU availability\n",
    "# print(torch.cuda.is_available())  # Should return True if the GPU is accessible\n",
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device(\"cuda\")  # Use the GPU if available\n",
    "#     print(torch.cuda.get_device_name(0))  # Should print the GPU name\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")  # Use CPU if no GPU is available\n",
    "\n",
    "# model_name = 'FacebookAI/xlm-roberta-base'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Task Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leboh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Map: 100%|██████████| 8804/8804 [00:01<00:00, 4992.34 examples/s]\n",
      "Map: 100%|██████████| 1100/1100 [00:00<00:00, 5307.07 examples/s]\n",
      "Map: 100%|██████████| 1101/1101 [00:00<00:00, 5385.07 examples/s]\n",
      "  9%|▉         | 500/5500 [4:49:30<48:14:25, 34.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5311, 'grad_norm': 27.3236141204834, 'learning_rate': 4.545454545454546e-05, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 1000/5500 [9:41:50<43:50:46, 35.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8204, 'grad_norm': 36.41783905029297, 'learning_rate': 4.0909090909090915e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 1368/5500 [13:24:35<46:43:46, 40.71s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 74\u001b[0m\n\u001b[0;32m     63\u001b[0m trainer_mlm \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     64\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_mlm,\n\u001b[0;32m     65\u001b[0m     args\u001b[38;5;241m=\u001b[39mpretraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \n\u001b[0;32m     71\u001b[0m )\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Pretrain the model\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m \u001b[43mtrainer_mlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leboh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leboh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2388\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2394\u001b[0m ):\n\u001b[0;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\leboh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:3518\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3516\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3518\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32mc:\\Users\\leboh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\accelerate\\accelerator.py:2246\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[0;32m   2245\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2246\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leboh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leboh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leboh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ======== Step 1: Pretraining on Unlabelled Data (Masked Language Model - MLM) ========\n",
    "\n",
    "# Load the XLM-RoBERTa tokenizer and model for MLM\n",
    "model_name = \"xlm-roberta-base\"  # Specify your model name here\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model_mlm = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_mlm = model_mlm.to(device)\n",
    "# Load unlabelled dataset (general text data for pretraining, e.g., headlines or articles)\n",
    "# unlabeled_dataset = load_dataset('text', data_files={'train': r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\health_transcripts1-xh.txt\"})  # Replace with your unlabelled data\n",
    "\n",
    "# Tokenize the unlabelled dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], return_special_tokens_mask=True, truncation=True, padding='max_length')\n",
    "\n",
    "tokenized_unlabeled_dataset = final_unlabeled_dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n",
    "\n",
    "# Custom function to compute accuracy and perplexity\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Masked tokens have label -100, ignore them in accuracy computation\n",
    "    mask = labels != -100\n",
    "    correct_predictions = (predictions == labels) & mask\n",
    "\n",
    "    accuracy = correct_predictions.sum() / mask.sum()\n",
    "    \n",
    "    # Perplexity calculation\n",
    "    loss = eval_pred.loss\n",
    "    perplexity = np.exp(loss)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'perplexity': perplexity\n",
    "    }\n",
    "\n",
    "# Create a data collator for MLM\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15  # Mask 15% of tokens\n",
    ")\n",
    "\n",
    "# Pretraining arguments\n",
    "pretraining_args = TrainingArguments(\n",
    "    output_dir='./pretrained_xlm_roberta',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=2,  # Reduced batch size\n",
    "    gradient_accumulation_steps=4,  # Gradient accumulation\n",
    "    fp16=True,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Initialize Trainer for MLM pretraining\n",
    "trainer_mlm = Trainer(\n",
    "    model=model_mlm,\n",
    "    args=pretraining_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_unlabeled_dataset['train'],\n",
    "    eval_dataset=tokenized_unlabeled_dataset['validation'],  # Use validation set for evaluation\n",
    "    compute_metrics=compute_metrics\n",
    "\n",
    ")\n",
    "\n",
    "# Pretrain the model\n",
    "trainer_mlm.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./pretrained_xlm_roberta\\\\tokenizer_config.json',\n",
       " './pretrained_xlm_roberta\\\\special_tokens_map.json',\n",
       " './pretrained_xlm_roberta\\\\sentencepiece.bpe.model',\n",
       " './pretrained_xlm_roberta\\\\added_tokens.json',\n",
       " './pretrained_xlm_roberta\\\\tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the pretrained MLM model\n",
    "model_mlm.save_pretrained('./pretrained_xlm_roberta')\n",
    "tokenizer.save_pretrained('./pretrained_xlm_roberta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\leboh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from seaborn) (2.1.0)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\leboh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\leboh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from seaborn) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\leboh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\leboh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\leboh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\leboh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\leboh\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\leboh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\leboh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\leboh\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\leboh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\leboh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\leboh\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install seaborn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specific Task Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "# from datasets import load_dataset\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # Load the pretrained tokenizer and model\n",
    "# tokenizer = AutoTokenizer.from_pretrained('./pretrained_xlm_roberta')\n",
    "# model_classifier = AutoModelForSequenceClassification.from_pretrained('./pretrained_xlm_roberta', num_labels=2)\n",
    "\n",
    "# # Load the dataset\n",
    "# dataset = load_dataset('csv', data_files={'train': 'train_data.csv', 'val': 'val_data.csv'})\n",
    "\n",
    "# # Encode the labels\n",
    "# label_encoder = LabelEncoder()\n",
    "# label_encoder.fit(['iindaba', 'ezemidlalo'])  # Adjust these labels if needed\n",
    "\n",
    "# def encode_labels(examples):\n",
    "#     examples['labels'] = label_encoder.transform(examples['label'])\n",
    "#     return examples\n",
    "\n",
    "# # Apply label encoding to the dataset\n",
    "# dataset = dataset.map(encode_labels, batched=True)\n",
    "\n",
    "# def tokenize_classification(examples):\n",
    "#     # Ensure 'headlines' is a list of strings\n",
    "#     headlines = examples['headlines']\n",
    "#     if isinstance(headlines, str):\n",
    "#         headlines = [headlines]\n",
    "    \n",
    "#     # Tokenize the headlines\n",
    "#     tokenized = tokenizer(\n",
    "#         headlines,\n",
    "#         padding=\"max_length\",\n",
    "#         truncation=True,\n",
    "#         max_length=512  # Adjust this value if needed\n",
    "#     )\n",
    "    \n",
    "#     return tokenized\n",
    "\n",
    "# # Apply the tokenizer function to the dataset\n",
    "# tokenized_labeled_dataset = dataset.map(\n",
    "#     tokenize_classification,\n",
    "#     batched=True,\n",
    "#     remove_columns=['headlines', 'label']\n",
    "# )\n",
    "\n",
    "# # Fine-tuning arguments\n",
    "# fine_tuning_args = TrainingArguments(\n",
    "#     output_dir='./finetuned_xlm_roberta',\n",
    "#     eval_strategy=\"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=4,\n",
    "#     per_device_eval_batch_size=4,\n",
    "#     gradient_accumulation_steps=4,  # Gradient accumulation\n",
    "#     fp16=True,\n",
    "#     num_train_epochs=5,\n",
    "#     weight_decay=0.01,\n",
    "# )\n",
    "\n",
    "# # Initialize Trainer for fine-tuning\n",
    "# trainer_classifier = Trainer(\n",
    "#     model=model_classifier,\n",
    "#     args=fine_tuning_args,\n",
    "#     train_dataset=tokenized_labeled_dataset['train'],\n",
    "#     eval_dataset=tokenized_labeled_dataset['val'],\n",
    "# )\n",
    "\n",
    "# # Fine-tune the model\n",
    "# trainer_classifier.train()\n",
    "\n",
    "# # Save the fine-tuned classifier model\n",
    "# model_classifier.save_pretrained('./finetuned_xlm_roberta')\n",
    "# tokenizer.save_pretrained('./finetuned_xlm_roberta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# from datasets import load_dataset\n",
    "# from sklearn.manifold import TSNE\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import plotly.express as px\n",
    "# from torch.utils.data import DataLoader\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # Load the fine-tuned model and tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained('./finetuned_xlm_roberta')\n",
    "# model_classifier = AutoModelForSequenceClassification.from_pretrained('./finetuned_xlm_roberta')\n",
    "\n",
    "# # Load the validation dataset\n",
    "# dataset = load_dataset('csv', data_files={'val': 'val_data.csv'})\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "# label_encoder.fit(['iindaba', 'ezemidlalo'])  # Adjust these labels if needed\n",
    "\n",
    "# # Encode the labels correctly\n",
    "# def encode_labels(examples):\n",
    "#     examples['labels'] = label_encoder.transform(examples['label'])  # Ensure 'label' is transformed into numeric form\n",
    "#     return examples\n",
    "\n",
    "# # Apply encoding to the dataset\n",
    "# dataset = dataset.map(encode_labels, batched=True)\n",
    "\n",
    "# # Tokenize the input dataset\n",
    "# def tokenize_classification(examples):\n",
    "#     return tokenizer(\n",
    "#         examples['headlines'],\n",
    "#         padding=\"max_length\",\n",
    "#         truncation=True,\n",
    "#         max_length=512,\n",
    "#         return_tensors='pt'  # Return PyTorch tensors\n",
    "#     )\n",
    "\n",
    "# # Apply tokenization to the validation set\n",
    "# tokenized_dataset = dataset['val'].map(tokenize_classification, batched=True)\n",
    "\n",
    "# # Convert to PyTorch tensors\n",
    "# tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# # Define a DataLoader for batching\n",
    "# batch_size = 16  # Adjust this based on your memory capacity\n",
    "# dataloader = DataLoader(tokenized_dataset, batch_size=batch_size)\n",
    "\n",
    "# # Move model to GPU if available\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model_classifier.to(device)\n",
    "\n",
    "# # Function to extract hidden states from the model\n",
    "# def extract_hidden_states(model, dataloader):\n",
    "#     model.eval()  # Set the model in evaluation mode\n",
    "#     hidden_states = []\n",
    "#     tokens = []  # Store the tokens (subwords)\n",
    "#     with torch.no_grad():\n",
    "#         for batch in dataloader:\n",
    "#             input_ids = batch['input_ids'].to(device)\n",
    "#             attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "#             # Get the model's output with hidden states\n",
    "#             outputs = model(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "            \n",
    "#             # Extract the last hidden layer\n",
    "#             last_hidden_state = outputs.hidden_states[-1]\n",
    "            \n",
    "#             # Mean pooling across the sequence length (dim=1)\n",
    "#             pooled_hidden_state = last_hidden_state.mean(dim=1)\n",
    "            \n",
    "#             hidden_states.append(pooled_hidden_state.cpu().numpy())\n",
    "            \n",
    "#             # Extract tokens for each input\n",
    "#             for batch_input_ids in input_ids:\n",
    "#                 tokens.append([tokenizer.decode(token_id) for token_id in batch_input_ids if token_id not in tokenizer.all_special_ids])\n",
    "\n",
    "#     return np.vstack(hidden_states), tokens  # Return hidden states and tokens\n",
    "\n",
    "# # Extract hidden states and tokens for the validation set\n",
    "# hidden_states, tokens = extract_hidden_states(model_classifier, dataloader)\n",
    "\n",
    "# # Apply t-SNE to reduce dimensionality to 2 components\n",
    "# tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)  # Adjust perplexity and n_iter if needed\n",
    "# tsne_result = tsne.fit_transform(hidden_states)\n",
    "\n",
    "# # Extract true labels from the validation dataset (make sure labels are numeric)\n",
    "# true_labels_tensor = torch.tensor(tokenized_dataset['labels'])\n",
    "# true_labels = true_labels_tensor.numpy()\n",
    "\n",
    "# # Prepare the token data for the plot\n",
    "# # Flatten the list of tokens so that each token corresponds to a t-SNE point\n",
    "# flat_tokens = [token for token_list in tokens for token in token_list]\n",
    "# flat_tsne_result = np.repeat(tsne_result, [len(token_list) for token_list in tokens], axis=0)\n",
    "# flat_labels = np.repeat(true_labels, [len(token_list) for token_list in tokens])\n",
    "\n",
    "# # Create a DataFrame with t-SNE results and token information\n",
    "# df_tsne = pd.DataFrame(flat_tsne_result, columns=['TSNE Component 1', 'TSNE Component 2'])\n",
    "# df_tsne['Label'] = flat_labels\n",
    "# df_tsne['Token'] = flat_tokens\n",
    "\n",
    "# # Create an interactive t-SNE plot using Plotly\n",
    "# fig = px.scatter(df_tsne, x='TSNE Component 1', y='TSNE Component 2',\n",
    "#                  color='Label',\n",
    "#                  hover_name='Token',  # Show token as hover information\n",
    "#                  title=\"t-SNE of Hidden States from XLM-RoBERTa (Tokens)\",\n",
    "#                  labels={'Label': 'Class'},\n",
    "#                  color_continuous_scale='Viridis')\n",
    "\n",
    "# # Show the figure\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Predictions and Evaluation (Specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n",
    "# from datasets import load_dataset\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Load the pretrained tokenizer and fine-tuned model\n",
    "# tokenizer = AutoTokenizer.from_pretrained('./finetuned_xlm_roberta')\n",
    "# model_classifier = AutoModelForSequenceClassification.from_pretrained('./finetuned_xlm_roberta')\n",
    "\n",
    "# # Load the test dataset\n",
    "# test_dataset = load_dataset('csv', data_files={'test': 'test_data.csv'})['test']\n",
    "\n",
    "# # Encode the labels using LabelEncoder\n",
    "# label_encoder = LabelEncoder()\n",
    "# label_encoder.fit(['iindaba', 'ezemidlalo'])  # Modify labels according to your dataset\n",
    "\n",
    "# def encode_labels(examples):\n",
    "#     examples['labels'] = label_encoder.transform(examples['label'])\n",
    "#     return examples\n",
    "\n",
    "# # Apply label encoding to the test dataset\n",
    "# test_dataset = test_dataset.map(encode_labels, batched=True)\n",
    "\n",
    "# # Tokenize the test dataset\n",
    "# def tokenize_classification(examples):\n",
    "#     return tokenizer(\n",
    "#         examples['headlines'],\n",
    "#         padding=\"max_length\",\n",
    "#         truncation=True,\n",
    "#         max_length=512\n",
    "#     )\n",
    "\n",
    "# tokenized_test_dataset = test_dataset.map(\n",
    "#     tokenize_classification,\n",
    "#     batched=True,\n",
    "#     remove_columns=['headlines', 'label']  # Remove original columns\n",
    "# )\n",
    "\n",
    "# # Initialize Trainer\n",
    "# trainer_classifier = Trainer(\n",
    "#     model=model_classifier,\n",
    "# )\n",
    "\n",
    "# # Make predictions on the tokenized test dataset\n",
    "# predictions = trainer_classifier.predict(tokenized_test_dataset)\n",
    "\n",
    "# # Extract logits and predicted class labels\n",
    "# logits = predictions.predictions\n",
    "# predicted_label_ids = np.argmax(logits, axis=-1)\n",
    "\n",
    "# # True labels (after tokenization)\n",
    "# true_labels = tokenized_test_dataset['labels']\n",
    "\n",
    "# # Generate confusion matrix\n",
    "# conf_matrix = confusion_matrix(true_labels, predicted_label_ids)\n",
    "\n",
    "# # Plot confusion matrix\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "#             xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "# plt.title(\"Confusion Matrix\")\n",
    "# plt.xlabel(\"Predicted Labels\")\n",
    "# plt.ylabel(\"True Labels\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained('FacebookAI/xlm-roberta-base')\n",
    "# base_model_classifier = AutoModelForSequenceClassification.from_pretrained('FacebookAI/xlm-roberta-base')\n",
    "\n",
    "# # Load the test dataset\n",
    "# test_dataset = load_dataset('csv', data_files={'test': 'test_data.csv'})['test']\n",
    "\n",
    "# # Encode the labels using LabelEncoder\n",
    "# label_encoder = LabelEncoder()\n",
    "# label_encoder.fit(['iindaba', 'ezemidlalo'])  # Modify labels according to your dataset\n",
    "\n",
    "# def encode_labels(examples):\n",
    "#     examples['labels'] = label_encoder.transform(examples['label'])\n",
    "#     return examples\n",
    "\n",
    "# # Apply label encoding to the test dataset\n",
    "# test_dataset = test_dataset.map(encode_labels, batched=True)\n",
    "\n",
    "# # Tokenize the test dataset\n",
    "# def tokenize_classification(examples):\n",
    "#     return tokenizer(\n",
    "#         examples['headlines'],\n",
    "#         padding=\"max_length\",\n",
    "#         truncation=True,\n",
    "#         max_length=512\n",
    "#     )\n",
    "\n",
    "# tokenized_test_dataset = test_dataset.map(\n",
    "#     tokenize_classification,\n",
    "#     batched=True,\n",
    "#     remove_columns=['headlines', 'label']  # Remove original columns\n",
    "# )\n",
    "\n",
    "# # Initialize Trainer\n",
    "# trainer_classifier = Trainer(\n",
    "#     model=base_model_classifier,\n",
    "# )\n",
    "\n",
    "# # Make predictions on the tokenized test dataset\n",
    "# predictions = trainer_classifier.predict(tokenized_test_dataset)\n",
    "\n",
    "# # Extract logits and predicted class labels\n",
    "# logits = predictions.predictions\n",
    "# predicted_label_ids = np.argmax(logits, axis=-1)\n",
    "\n",
    "# # True labels (after tokenization)\n",
    "# true_labels = tokenized_test_dataset['labels']\n",
    "\n",
    "# # Generate confusion matrix\n",
    "# conf_matrix = confusion_matrix(true_labels, predicted_label_ids)\n",
    "\n",
    "# # Plot confusion matrix\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "#             xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "# plt.title(\"Confusion Matrix\")\n",
    "# plt.xlabel(\"Predicted Labels\")\n",
    "# plt.ylabel(\"True Labels\")\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
