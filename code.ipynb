{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mdhla\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the English file using read_fwf\n",
    "df = pd.read_fwf(r\"data\\TranslatedForMeMaT\\health_transcripts1-xh.txt\", \n",
    "                    header=None)\n",
    "\n",
    "\n",
    "df.columns = ['text']  # Naming the column as 'text'\n",
    "# Step 2: Convert pandas DataFrame to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ingaba zikhona izinto ezinokwenziwa ukuthomala...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kubalulekile ukuzilawula izinto ezingumngciphe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abantu abanesifo seswekile basemngciphekweni o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ingaba sisenokulungiseka?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wakhe wahlaselwa sisifo sentliziyo?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Eyona ndlela isebenzayo yokuthintela isandulel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Xa ikholesteroli embi ekwabizwa ngokuba yiLDL ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ndinekholesteroli ephezulu kunye noxinzelelo l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Utata wayenesifo sentliziyo kunye neswekile wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Indlela enditya nendizilolonga ngayo intle.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Xa usapho lwakho lunembali yesifo seswekile ku...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Qhubekeka nento entle oyenzayo yokutya kakuhle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Ndinexhala.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Ingaba umntu onesifo seswekile esiluHlobo 1 ok...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Umama wam unesifo seswekile yaye kuDisemba won...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Akatyebanga ngokugqithisileyo yaye utya amayez...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Ngawaphi amanyathelo ekufuneka awathathe?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Utya ukutya okunamafutha amancinane.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Ingaba kukho amayeza athile ekufanel'ukuba uya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Utata wam wahlaselwa sisifo sentliziyo esasiba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Bendisesibhedlele ndiyotyandwa intliziyo ndabe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Kuqhelekile oku?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Kuza kuthatha ixesha elingakanani ukuba isweki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Mhlawumbi ezili-10 ngemini zilungile.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Umama wam unentliziyo eminyeneyo engasebenzi k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Ukwanalo noHlobo 2 lwesifo seswekile yaye useb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Iyaphucuka iswekile yakhe.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Kufuneka azilolonge.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Ndineminyaka engama-45 ubudala yaye ndinesifo ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Mvanje ndiziva ndiba ndindisholo kwesi sandla ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text\n",
       "0   Ingaba zikhona izinto ezinokwenziwa ukuthomala...\n",
       "1   Kubalulekile ukuzilawula izinto ezingumngciphe...\n",
       "2   Abantu abanesifo seswekile basemngciphekweni o...\n",
       "3                           Ingaba sisenokulungiseka?\n",
       "4                 Wakhe wahlaselwa sisifo sentliziyo?\n",
       "5   Eyona ndlela isebenzayo yokuthintela isandulel...\n",
       "6   Xa ikholesteroli embi ekwabizwa ngokuba yiLDL ...\n",
       "7   Ndinekholesteroli ephezulu kunye noxinzelelo l...\n",
       "8   Utata wayenesifo sentliziyo kunye neswekile wa...\n",
       "9         Indlela enditya nendizilolonga ngayo intle.\n",
       "10  Xa usapho lwakho lunembali yesifo seswekile ku...\n",
       "11  Qhubekeka nento entle oyenzayo yokutya kakuhle...\n",
       "12                                        Ndinexhala.\n",
       "13  Ingaba umntu onesifo seswekile esiluHlobo 1 ok...\n",
       "14  Umama wam unesifo seswekile yaye kuDisemba won...\n",
       "15  Akatyebanga ngokugqithisileyo yaye utya amayez...\n",
       "16          Ngawaphi amanyathelo ekufuneka awathathe?\n",
       "17               Utya ukutya okunamafutha amancinane.\n",
       "18  Ingaba kukho amayeza athile ekufanel'ukuba uya...\n",
       "19  Utata wam wahlaselwa sisifo sentliziyo esasiba...\n",
       "20  Bendisesibhedlele ndiyotyandwa intliziyo ndabe...\n",
       "21                                   Kuqhelekile oku?\n",
       "22  Kuza kuthatha ixesha elingakanani ukuba isweki...\n",
       "23              Mhlawumbi ezili-10 ngemini zilungile.\n",
       "24  Umama wam unentliziyo eminyeneyo engasebenzi k...\n",
       "25  Ukwanalo noHlobo 2 lwesifo seswekile yaye useb...\n",
       "26                         Iyaphucuka iswekile yakhe.\n",
       "27                               Kufuneka azilolonge.\n",
       "28  Ndineminyaka engama-45 ubudala yaye ndinesifo ...\n",
       "29  Mvanje ndiziva ndiba ndindisholo kwesi sandla ..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdhla\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ede334020ce4409bdf078c7b72b13fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/508 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 3: Load Pretrained Tokenizer\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "# Step 4: Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# Tokenizing the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, remove_columns=[\"text\"])\n",
    "\n",
    "# Step 5: Define Data Collator for Masked Language Modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, \n",
    "    mlm=True, \n",
    "    mlm_probability=0.15  # Mask 15% of the tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA RTX A1000 6GB Laptop GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741145406f16466592514b7bcd02c8ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1140.4688, 'train_samples_per_second': 1.336, 'train_steps_per_second': 0.168, 'train_loss': 3.9301487604777017, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./fine-tuned-xlm-roberta-mlm\\\\tokenizer_config.json',\n",
       " './fine-tuned-xlm-roberta-mlm\\\\special_tokens_map.json',\n",
       " './fine-tuned-xlm-roberta-mlm\\\\sentencepiece.bpe.model',\n",
       " './fine-tuned-xlm-roberta-mlm\\\\added_tokens.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Check GPU availability\n",
    "print(torch.cuda.is_available())  # Should return True if the GPU is accessible\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Use the GPU if available\n",
    "    print(torch.cuda.get_device_name(0))  # Should print the GPU name\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Use CPU if no GPU is available\n",
    "\n",
    "# Step 2: Load Pretrained XLM-RoBERTa Model for Masked Language Modeling\n",
    "model = XLMRobertaForMaskedLM.from_pretrained(\"xlm-roberta-base\").to(device)\n",
    "\n",
    "# Step 5: Set up Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,  # Adjust based on GPU memory\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=200,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "# Step 6: Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets,  # Tokenized dataset\n",
    ")\n",
    "\n",
    "# Step 7: Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Step 8: Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained(\"./fine-tuned-xlm-roberta-mlm\")\n",
    "tokenizer.save_pretrained(\"./fine-tuned-xlm-roberta-mlm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Specific Fine Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
