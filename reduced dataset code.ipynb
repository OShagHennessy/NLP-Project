{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import XLMRobertaTokenizer, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import Dataset, load_dataset, DatasetDict\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenation complete. Saved to C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\concatenated_transcripts_reduced.txt\n"
     ]
    }
   ],
   "source": [
    "# Example: List of files to concatenate\n",
    "files_to_concatenate = [\n",
    "    r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\health_transcripts1-xh.txt\",\n",
    "    r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\health_transcripts2-xh.txt\",\n",
    "    r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\health_transcripts3-xh.txt\",\n",
    "    r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\mobile_xhosa_mono-xh.txt\",\n",
    "    # r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_aa-xh.txt\",\n",
    "    # r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_ab-xh.txt\",\n",
    "    # r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_ac-xh.txt\",\n",
    "    # r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_ad-xh.txt\",\n",
    "    # r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_ae-xh.txt\",\n",
    "    # r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_af-xh.txt\",\n",
    "    # r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_ag-xh.txt\",\n",
    "    # r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_ah-xh.txt\",\n",
    "    # r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_ai-xh.txt\",\n",
    "    # r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_aj-xh.txt\",\n",
    "    # r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_ak-xh.txt\",\n",
    "    # r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_al-xh.txt\",\n",
    "    # r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_am-xh.txt\",\n",
    "    # r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_an-xh.txt\",\n",
    "    # r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_ao-xh.txt\",\n",
    "    # r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_ap-xh.txt\",\n",
    "    # r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_aq-xh.txt\",\n",
    "    # r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_ar-xh.txt\",\n",
    "    # r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_as-xh.txt\",\n",
    "    # r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_at-xh.txt\",\n",
    "    # r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_au-xh.txt\",\n",
    "    # r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_av-xh.txt\",\n",
    "    # r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_aw-xh.txt\",\n",
    "    # r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\transcripts_auto_ax-xh.txt\",\n",
    "    # r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\General datasets\\westerncape-health-xh.txt\",\n",
    "]\n",
    "\n",
    "# Initialize an empty string to store the concatenated content\n",
    "concatenated_content = \"\"\n",
    "\n",
    "# Loop through each file and concatenate their content\n",
    "for file_path in files_to_concatenate:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        concatenated_content += file.read() + \"\\n\"  # Add a newline between files if desired\n",
    "\n",
    "# Save the concatenated content to a new file\n",
    "output_file = r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\concatenated_transcripts_reduced.txt\"\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    file.write(concatenated_content)\n",
    "\n",
    "print(f\"Concatenation complete. Saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating full split: 1112 examples [00:00, 10548.06 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    full: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1112\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load unlabelled dataset (general text data for pretraining, e.g., headlines or articles)\n",
    "unlabeled_dataset = load_dataset('text', data_files={'full': r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\concatenated_transcripts_reduced.txt\"})  # Replace with your unlabelled data\n",
    "unlabeled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Ingaba zikhona izinto ezinokwenziwa ukuthomalalisa iimpawu ngaphandle kokusebenzisa amayeza?', 'Kubalulekile ukuzilawula izinto ezingumngcipheko kwangoko. ', 'Abantu abanesifo seswekile basemngciphekweni othe chatha wokuphathwa sisifo sentliziyo apha ebomini babo. ', 'Ingaba sisenokulungiseka?', 'Wakhe wahlaselwa sisifo sentliziyo?']}\n"
     ]
    }
   ],
   "source": [
    "print(unlabeled_dataset['train'][0:5])  # This will display the first 5 rows of the 'train' split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 889\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 111\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 112\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into train, validation, and test sets (e.g., 80% train, 10% validation, 10% test)\n",
    "unlabeled_dataset = unlabeled_dataset['train'].train_test_split(test_size=0.2)  # 80% train, 20% test\n",
    "\n",
    "# Split the test set further into validation and test (50% of 20% -> 10% validation, 10% test)\n",
    "validation_test_split = unlabeled_dataset['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "# Combine the splits into a DatasetDict\n",
    "final_unlabeled_dataset = DatasetDict({\n",
    "    'train': unlabeled_dataset['train'],\n",
    "    'validation': validation_test_split['train'],\n",
    "    'test': validation_test_split['test']\n",
    "})\n",
    "\n",
    "# Display the structure of the final split dataset\n",
    "print(final_unlabeled_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specific Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from selenium.common.exceptions import TimeoutException, NoSuchElementException, ElementClickInterceptedException\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# import pandas as pd\n",
    "# from urllib.parse import urlparse\n",
    "\n",
    "# def extract_label_from_url(url):\n",
    "#     parsed_url = urlparse(url)\n",
    "#     path = parsed_url.path\n",
    "#     label = path.split('/')[-2]  # Use the second-to-last part of the path\n",
    "#     return label\n",
    "\n",
    "# def scrape_headlines_selenium(url, num_headlines=5000):\n",
    "#     chrome_options = Options()\n",
    "#     chrome_options.add_argument(\"--headless\")\n",
    "#     service = Service(ChromeDriverManager().install())\n",
    "#     driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "#     try:\n",
    "#         driver.get(url)\n",
    "#         print(f\"Page title: {driver.title}\")\n",
    "\n",
    "#         headlines = []\n",
    "#         load_more_attempts = 0\n",
    "#         max_load_more_attempts = 10  # Adjust this value if needed\n",
    "\n",
    "#         while len(headlines) < num_headlines and load_more_attempts < max_load_more_attempts:\n",
    "#             # Wait for articles to load\n",
    "#             WebDriverWait(driver, 10).until(\n",
    "#                 EC.presence_of_element_located((By.XPATH, \"//div[contains(@class, 'td-block-span6')]\"))\n",
    "#             )\n",
    "\n",
    "#             # Find all article elements\n",
    "#             articles = driver.find_elements(By.XPATH, \"//div[contains(@class, 'td-block-span6')]\")\n",
    "\n",
    "#             # Extract headlines from new articles\n",
    "#             for article in articles[len(headlines):]:\n",
    "#                 try:\n",
    "#                     headline_element = article.find_element(By.XPATH, \".//div[contains(@class, 'td-module-thumb')]/a\")\n",
    "#                     headline = headline_element.get_attribute('title').strip()\n",
    "#                     if headline not in headlines:\n",
    "#                         headlines.append(headline)\n",
    "#                         # print(f\"Found headline: {headline}\")\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error extracting headline: {e}\")\n",
    "\n",
    "#             print(f\"Total headlines found: {len(headlines)}\")\n",
    "\n",
    "#             if len(headlines) >= num_headlines:\n",
    "#                 break\n",
    "\n",
    "#             # Try to click 'Load more' button\n",
    "#             load_more_clicked = False\n",
    "#             try:\n",
    "#                 # Scroll to the bottom of the page\n",
    "#                 driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "#                 time.sleep(2)  # Wait for any lazy-loaded content\n",
    "\n",
    "#                 # Try to find the 'Load more' button\n",
    "#                 load_more_button = WebDriverWait(driver, 5).until(\n",
    "#                     EC.element_to_be_clickable((By.ID, \"load_more_articles_button\"))\n",
    "#                 )\n",
    "#                 driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", load_more_button)\n",
    "#                 time.sleep(1)  # Short pause after scrolling\n",
    "#                 load_more_button.click()\n",
    "#                 print(\"Clicked 'Load more' button\")\n",
    "#                 load_more_clicked = True\n",
    "#                 time.sleep(3)  # Wait for new content to load\n",
    "#             except (TimeoutException, NoSuchElementException, ElementClickInterceptedException) as e:\n",
    "#                 print(f\"Could not click 'Load more' button: {e}\")\n",
    "                \n",
    "#                 # Try alternative method using JavaScript click\n",
    "#                 try:\n",
    "#                     driver.execute_script(\"document.getElementById('load_more_articles_button').click();\")\n",
    "#                     print(\"Clicked 'Load more' button using JavaScript\")\n",
    "#                     load_more_clicked = True\n",
    "#                     time.sleep(3)  # Wait for new content to load\n",
    "#                 except Exception as js_e:\n",
    "#                     print(f\"Could not click 'Load more' button using JavaScript: {js_e}\")\n",
    "\n",
    "#             if not load_more_clicked:\n",
    "#                 load_more_attempts += 1\n",
    "#                 print(f\"Load more attempt {load_more_attempts} failed\")\n",
    "#             else:\n",
    "#                 load_more_attempts = 0  # Reset the counter if we successfully loaded more\n",
    "\n",
    "#         if load_more_attempts >= max_load_more_attempts:\n",
    "#             print(\"Reached maximum 'Load more' attempts. Ending search.\")\n",
    "\n",
    "#         return headlines[:num_headlines]\n",
    "\n",
    "#     finally:\n",
    "#         driver.quit()\n",
    "\n",
    "# # URL of the webpage\n",
    "# url = 'https://www.isolezwelesixhosa.co.za/ezemidlalo/'\n",
    "\n",
    "# print(\"Attempting to scrape with Selenium:\")\n",
    "# headlines = scrape_headlines_selenium(url)\n",
    "\n",
    "# # Extract label from URL\n",
    "# label = extract_label_from_url(url)\n",
    "\n",
    "# # Create DataFrame with headlines and label\n",
    "# df = pd.DataFrame({\n",
    "#     'headlines': headlines,\n",
    "#     'label': [label] * len(headlines)\n",
    "# })\n",
    "\n",
    "# # Save the headlines to a CSV file\n",
    "# df.to_csv('headlines.csv', index=False)\n",
    "\n",
    "# print(f\"\\n{len(headlines)} Headlines saved to headlines.csv\")\n",
    "# print(f\"Label used: {label}\")\n",
    "\n",
    "# # Print the first few headlines for verification\n",
    "# print(\"\\nFirst few headlines:\")\n",
    "# for headline in headlines[:5]:\n",
    "#     print(f\"{headline} (Label: {label})\")\n",
    "\n",
    "# # Print the last few headlines for verification\n",
    "# print(\"\\nLast few headlines:\")\n",
    "# for headline in headlines[-5:]:\n",
    "#     print(f\"{headline} (Label: {label})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # URL of the webpage\n",
    "# url = 'https://www.isolezwelesixhosa.co.za/iindaba/'\n",
    "\n",
    "# print(\"Attempting to scrape with Selenium:\")\n",
    "# headlines = scrape_headlines_selenium(url)\n",
    "\n",
    "# # Extract label from URL\n",
    "# label = extract_label_from_url(url)\n",
    "\n",
    "# # Create DataFrame with headlines and label\n",
    "# df = pd.DataFrame({\n",
    "#     'headlines': headlines,\n",
    "#     'label': [label] * len(headlines)\n",
    "# })\n",
    "\n",
    "# # Save the headlines to a CSV file\n",
    "# df.to_csv('more_headlines.csv', index=False)\n",
    "\n",
    "# print(f\"\\n{len(headlines)} Headlines saved to more_headlines.csv\")\n",
    "# print(f\"Label used: {label}\")\n",
    "\n",
    "# # Print the first few headlines for verification\n",
    "# print(\"\\nFirst few headlines:\")\n",
    "# for headline in headlines[:5]:\n",
    "#     print(f\"{headline} (Label: {label})\")\n",
    "\n",
    "# # Print the last few headlines for verification\n",
    "# print(\"\\nLast few headlines:\")\n",
    "# for headline in headlines[-5:]:\n",
    "#     print(f\"{headline} (Label: {label})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1 = pd.read_csv(\"headlines.csv\")\n",
    "# data2 = pd.read_csv(\"more_headlines.csv\")\n",
    "\n",
    "# df = pd.concat([data1, data2], ignore_index=True)\n",
    "# df.dropna(inplace=True)\n",
    "# df.to_csv(\"xhosa headlines dataset.csv\")\n",
    "\n",
    "\n",
    "# import re\n",
    "# import pandas as pd\n",
    "\n",
    "# # Function to remove punctuation from a single string\n",
    "# def remove_punctuation(text):\n",
    "#     # Check if the input is a string, otherwise return it unchanged\n",
    "#     if isinstance(text, str):\n",
    "#         pattern = r'[^\\w\\s]'  # Regex pattern to match any punctuation\n",
    "#         return re.sub(pattern, '', text)\n",
    "\n",
    "\n",
    "# # Apply the remove_punctuation function to the 'headlines' column\n",
    "# df['headlines'] = df['headlines'].apply(remove_punctuation)\n",
    "\n",
    "# # Display the updated DataFrame\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Step 1: Split 85% for train + test and 15% for validation\n",
    "# train_test_df, val_df = train_test_split(df, test_size=0.15, random_state=42)\n",
    "\n",
    "# # Step 2: Split train_test_df into 70% training and 15% testing (equivalent to 85% * 70/85 for training)\n",
    "# train_df, test_df = train_test_split(train_test_df, test_size=0.1765, random_state=42)  # 0.1765 ≈ 15 / 85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.to_csv(\"train_data.csv\", index=False)\n",
    "# test_df.to_csv(\"test_data.csv\", index=False)\n",
    "# val_df.to_csv(\"val_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# # Function to remove punctuation from a single string\n",
    "# def remove_punctuation(text):\n",
    "#     # Check if the input is a string, otherwise return it unchanged\n",
    "#     if isinstance(text, str):\n",
    "#         pattern = r'[^\\w\\s]'  # Regex pattern to match any punctuation\n",
    "#         return re.sub(pattern, '', text)\n",
    "\n",
    "# # Function to remove punctuation from a text file\n",
    "# def remove_punctuation_from_file(file_path):\n",
    "#     try:\n",
    "#         # Open the file with UTF-8 encoding and read its contents\n",
    "#         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#             text = file.read()\n",
    "\n",
    "#         # Remove punctuation from the entire text\n",
    "#         clean_text = remove_punctuation(text)\n",
    "\n",
    "#         # Save the clean text back to a file (you can overwrite the original file or save to a new file)\n",
    "#         with open('cleaned_' + file_path, 'w', encoding='utf-8') as clean_file:\n",
    "#             clean_file.write(clean_text)\n",
    "#     except UnicodeDecodeError:\n",
    "#         print(f\"Error: Unable to read the file {file_path} due to encoding issues.\")\n",
    "\n",
    "# # Example usage\n",
    "# file_path = 'xhosa constitution.txt'  # Replace with your text file path\n",
    "# remove_punctuation_from_file(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Check GPU availability\n",
    "# print(torch.cuda.is_available())  # Should return True if the GPU is accessible\n",
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device(\"cuda\")  # Use the GPU if available\n",
    "#     print(torch.cuda.get_device_name(0))  # Should print the GPU name\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")  # Use CPU if no GPU is available\n",
    "\n",
    "# model_name = 'FacebookAI/xlm-roberta-base'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Task Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\n",
      "\u001b[A\n",
      "Map: 100%|██████████| 889/889 [00:00<00:00, 4957.67 examples/s]\n",
      "\n",
      "Map: 100%|██████████| 111/111 [00:00<00:00, 2341.08 examples/s]\n",
      "\n",
      "Map: 100%|██████████| 112/112 [00:00<00:00, 2383.02 examples/s]\n",
      "  1%|▏         | 38/2715 [28:04<32:57:45, 44.33s/it]\n",
      " 90%|█████████ | 500/555 [7:14:23<12:22:02, 809.50s/it] \n",
      " 90%|█████████ | 500/555 [7:14:23<12:22:02, 809.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6874, 'grad_norm': 47.502899169921875, 'learning_rate': 4.954954954954955e-06, 'epoch': 4.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 555/555 [8:14:32<00:00, 58.96s/it]    \n",
      "100%|██████████| 555/555 [8:14:39<00:00, 53.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 29679.2123, 'train_samples_per_second': 0.15, 'train_steps_per_second': 0.019, 'train_loss': 3.6100425926414696, 'epoch': 4.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=555, training_loss=3.6100425926414696, metrics={'train_runtime': 29679.2123, 'train_samples_per_second': 0.15, 'train_steps_per_second': 0.019, 'total_flos': 1170567489576960.0, 'train_loss': 3.6100425926414696, 'epoch': 4.98876404494382})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ======== Step 1: Pretraining on Unlabelled Data (Masked Language Model - MLM) ========\n",
    "\n",
    "# Load the XLM-RoBERTa tokenizer and model for MLM\n",
    "model_name = \"xlm-roberta-base\"  # Specify your model name here\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model_mlm = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_mlm = model_mlm.to(device)\n",
    "# Load unlabelled dataset (general text data for pretraining, e.g., headlines or articles)\n",
    "# unlabeled_dataset = load_dataset('text', data_files={'train': r\"C:\\Users\\leboh\\OneDrive - University of Witwatersrand\\Documents\\GitHub\\NLP-Project\\health_transcripts1-xh.txt\"})  # Replace with your unlabelled data\n",
    "\n",
    "# Tokenize the unlabelled dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], return_special_tokens_mask=True, truncation=True, padding='max_length')\n",
    "\n",
    "tokenized_unlabeled_dataset = final_unlabeled_dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n",
    "\n",
    "# Custom function to compute accuracy and perplexity\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Masked tokens have label -100, ignore them in accuracy computation\n",
    "    mask = labels != -100\n",
    "    correct_predictions = (predictions == labels) & mask\n",
    "\n",
    "    accuracy = correct_predictions.sum() / mask.sum()\n",
    "    \n",
    "    # Perplexity calculation\n",
    "    loss = eval_pred.loss\n",
    "    perplexity = np.exp(loss)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'perplexity': perplexity\n",
    "    }\n",
    "\n",
    "# Create a data collator for MLM\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15  # Mask 15% of tokens\n",
    ")\n",
    "\n",
    "# Pretraining arguments\n",
    "pretraining_args = TrainingArguments(\n",
    "    output_dir='./pretrained_xlm_roberta',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=2,  # Reduced batch size\n",
    "    gradient_accumulation_steps=4,  # Gradient accumulation\n",
    "    fp16=True,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Initialize Trainer for MLM pretraining\n",
    "trainer_mlm = Trainer(\n",
    "    model=model_mlm,\n",
    "    args=pretraining_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_unlabeled_dataset['train'],\n",
    "    eval_dataset=tokenized_unlabeled_dataset['validation'],  # Use validation set for evaluation\n",
    "    compute_metrics=compute_metrics\n",
    "\n",
    ")\n",
    "\n",
    "# Pretrain the model\n",
    "trainer_mlm.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./pretrained_xlm_roberta\\\\tokenizer_config.json',\n",
       " './pretrained_xlm_roberta\\\\special_tokens_map.json',\n",
       " './pretrained_xlm_roberta\\\\sentencepiece.bpe.model',\n",
       " './pretrained_xlm_roberta\\\\added_tokens.json',\n",
       " './pretrained_xlm_roberta\\\\tokenizer.json')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the pretrained MLM model\n",
    "model_mlm.save_pretrained('./pretrained_xlm_roberta')\n",
    "tokenizer.save_pretrained('./pretrained_xlm_roberta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\leboh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from seaborn) (2.1.0)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\leboh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\leboh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from seaborn) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\leboh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\leboh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\leboh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\leboh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\leboh\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\leboh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\leboh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\leboh\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\leboh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\leboh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\leboh\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specific Task Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ./pretrained_xlm_roberta and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Unable to find 'C:/Users/leboh/OneDrive - University of Witwatersrand/Desktop\\train_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m model_classifier \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./pretrained_xlm_roberta\u001b[39m\u001b[38;5;124m'\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcsv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_data.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_data.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Encode the labels\u001b[39;00m\n\u001b[0;32m     13\u001b[0m label_encoder \u001b[38;5;241m=\u001b[39m LabelEncoder()\n",
      "File \u001b[1;32mc:\\Users\\leboh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\load.py:2074\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[0;32m   2069\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[0;32m   2070\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[0;32m   2071\u001b[0m )\n\u001b[0;32m   2073\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[1;32m-> 2074\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2087\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2088\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2089\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2091\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[0;32m   2092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[1;32mc:\\Users\\leboh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\load.py:1795\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[0;32m   1793\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[0;32m   1794\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[1;32m-> 1795\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1804\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1805\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_custom_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1807\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[0;32m   1808\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[1;32mc:\\Users\\leboh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\load.py:1551\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[0;32m   1528\u001b[0m \u001b[38;5;66;03m# We have several ways to get a dataset builder:\u001b[39;00m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   1530\u001b[0m \u001b[38;5;66;03m# - if path is the name of a packaged dataset module\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1542\u001b[0m \n\u001b[0;32m   1543\u001b[0m \u001b[38;5;66;03m# Try packaged\u001b[39;00m\n\u001b[0;32m   1544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES:\n\u001b[0;32m   1545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPackagedDatasetModuleFactory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 1551\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;66;03m# Try locally\u001b[39;00m\n\u001b[0;32m   1553\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mendswith(filename):\n",
      "File \u001b[1;32mc:\\Users\\leboh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\load.py:935\u001b[0m, in \u001b[0;36mPackagedDatasetModuleFactory.get_module\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    929\u001b[0m base_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mexpanduser()\u001b[38;5;241m.\u001b[39mresolve()\u001b[38;5;241m.\u001b[39mas_posix()\n\u001b[0;32m    930\u001b[0m patterns \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    931\u001b[0m     sanitize_patterns(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files)\n\u001b[0;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m get_data_patterns(base_path, download_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_config)\n\u001b[0;32m    934\u001b[0m )\n\u001b[1;32m--> 935\u001b[0m data_files \u001b[38;5;241m=\u001b[39m \u001b[43mDataFilesDict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_patterns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    939\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    940\u001b[0m supports_metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01min\u001b[39;00m _MODULE_SUPPORTS_METADATA\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m supports_metadata \u001b[38;5;129;01mand\u001b[39;00m patterns \u001b[38;5;241m!=\u001b[39m DEFAULT_PATTERNS_ALL:\n",
      "File \u001b[1;32mc:\\Users\\leboh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\data_files.py:721\u001b[0m, in \u001b[0;36mDataFilesDict.from_patterns\u001b[1;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[0;32m    716\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m()\n\u001b[0;32m    717\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, patterns_for_key \u001b[38;5;129;01min\u001b[39;00m patterns\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    718\u001b[0m     out[key] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    719\u001b[0m         patterns_for_key\n\u001b[0;32m    720\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(patterns_for_key, DataFilesList)\n\u001b[1;32m--> 721\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mDataFilesList\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_patterns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatterns_for_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    727\u001b[0m     )\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\leboh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\data_files.py:624\u001b[0m, in \u001b[0;36mDataFilesList.from_patterns\u001b[1;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m patterns:\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    623\u001b[0m         data_files\u001b[38;5;241m.\u001b[39mextend(\n\u001b[1;32m--> 624\u001b[0m             \u001b[43mresolve_pattern\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m                \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m         )\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_magic(pattern):\n",
      "File \u001b[1;32mc:\\Users\\leboh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\data_files.py:411\u001b[0m, in \u001b[0;36mresolve_pattern\u001b[1;34m(pattern, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m allowed_extensions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    410\u001b[0m         error_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m with any supported extension \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(allowed_extensions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 411\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(error_msg)\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Unable to find 'C:/Users/leboh/OneDrive - University of Witwatersrand/Desktop\\train_data.csv'"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "# from datasets import load_dataset\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # Load the pretrained tokenizer and model\n",
    "# tokenizer = AutoTokenizer.from_pretrained('./pretrained_xlm_roberta')\n",
    "# model_classifier = AutoModelForSequenceClassification.from_pretrained('./pretrained_xlm_roberta', num_labels=2)\n",
    "\n",
    "# # Load the dataset\n",
    "# dataset = load_dataset('csv', data_files={'train': 'train_data.csv', 'val': 'val_data.csv'})\n",
    "\n",
    "# # Encode the labels\n",
    "# label_encoder = LabelEncoder()\n",
    "# label_encoder.fit(['iindaba', 'ezemidlalo'])  # Adjust these labels if needed\n",
    "\n",
    "# def encode_labels(examples):\n",
    "#     examples['labels'] = label_encoder.transform(examples['label'])\n",
    "#     return examples\n",
    "\n",
    "# # Apply label encoding to the dataset\n",
    "# dataset = dataset.map(encode_labels, batched=True)\n",
    "\n",
    "# def tokenize_classification(examples):\n",
    "#     # Ensure 'headlines' is a list of strings\n",
    "#     headlines = examples['headlines']\n",
    "#     if isinstance(headlines, str):\n",
    "#         headlines = [headlines]\n",
    "    \n",
    "#     # Tokenize the headlines\n",
    "#     tokenized = tokenizer(\n",
    "#         headlines,\n",
    "#         padding=\"max_length\",\n",
    "#         truncation=True,\n",
    "#         max_length=512  # Adjust this value if needed\n",
    "#     )\n",
    "    \n",
    "#     return tokenized\n",
    "\n",
    "# # Apply the tokenizer function to the dataset\n",
    "# tokenized_labeled_dataset = dataset.map(\n",
    "#     tokenize_classification,\n",
    "#     batched=True,\n",
    "#     remove_columns=['headlines', 'label']\n",
    "# )\n",
    "\n",
    "# # Fine-tuning arguments\n",
    "# fine_tuning_args = TrainingArguments(\n",
    "#     output_dir='./finetuned_xlm_roberta',\n",
    "#     eval_strategy=\"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=4,\n",
    "#     per_device_eval_batch_size=4,\n",
    "#     gradient_accumulation_steps=4,  # Gradient accumulation\n",
    "#     fp16=True,\n",
    "#     num_train_epochs=5,\n",
    "#     weight_decay=0.01,\n",
    "# )\n",
    "\n",
    "# # Initialize Trainer for fine-tuning\n",
    "# trainer_classifier = Trainer(\n",
    "#     model=model_classifier,\n",
    "#     args=fine_tuning_args,\n",
    "#     train_dataset=tokenized_labeled_dataset['train'],\n",
    "#     eval_dataset=tokenized_labeled_dataset['val'],\n",
    "# )\n",
    "\n",
    "# # Fine-tune the model\n",
    "# trainer_classifier.train()\n",
    "\n",
    "# # Save the fine-tuned classifier model\n",
    "# model_classifier.save_pretrained('./finetuned_xlm_roberta')\n",
    "# tokenizer.save_pretrained('./finetuned_xlm_roberta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# from datasets import load_dataset\n",
    "# from sklearn.manifold import TSNE\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import plotly.express as px\n",
    "# from torch.utils.data import DataLoader\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # Load the fine-tuned model and tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained('./finetuned_xlm_roberta')\n",
    "# model_classifier = AutoModelForSequenceClassification.from_pretrained('./finetuned_xlm_roberta')\n",
    "\n",
    "# # Load the validation dataset\n",
    "# dataset = load_dataset('csv', data_files={'val': 'val_data.csv'})\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "# label_encoder.fit(['iindaba', 'ezemidlalo'])  # Adjust these labels if needed\n",
    "\n",
    "# # Encode the labels correctly\n",
    "# def encode_labels(examples):\n",
    "#     examples['labels'] = label_encoder.transform(examples['label'])  # Ensure 'label' is transformed into numeric form\n",
    "#     return examples\n",
    "\n",
    "# # Apply encoding to the dataset\n",
    "# dataset = dataset.map(encode_labels, batched=True)\n",
    "\n",
    "# # Tokenize the input dataset\n",
    "# def tokenize_classification(examples):\n",
    "#     return tokenizer(\n",
    "#         examples['headlines'],\n",
    "#         padding=\"max_length\",\n",
    "#         truncation=True,\n",
    "#         max_length=512,\n",
    "#         return_tensors='pt'  # Return PyTorch tensors\n",
    "#     )\n",
    "\n",
    "# # Apply tokenization to the validation set\n",
    "# tokenized_dataset = dataset['val'].map(tokenize_classification, batched=True)\n",
    "\n",
    "# # Convert to PyTorch tensors\n",
    "# tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# # Define a DataLoader for batching\n",
    "# batch_size = 16  # Adjust this based on your memory capacity\n",
    "# dataloader = DataLoader(tokenized_dataset, batch_size=batch_size)\n",
    "\n",
    "# # Move model to GPU if available\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model_classifier.to(device)\n",
    "\n",
    "# # Function to extract hidden states from the model\n",
    "# def extract_hidden_states(model, dataloader):\n",
    "#     model.eval()  # Set the model in evaluation mode\n",
    "#     hidden_states = []\n",
    "#     tokens = []  # Store the tokens (subwords)\n",
    "#     with torch.no_grad():\n",
    "#         for batch in dataloader:\n",
    "#             input_ids = batch['input_ids'].to(device)\n",
    "#             attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "#             # Get the model's output with hidden states\n",
    "#             outputs = model(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "            \n",
    "#             # Extract the last hidden layer\n",
    "#             last_hidden_state = outputs.hidden_states[-1]\n",
    "            \n",
    "#             # Mean pooling across the sequence length (dim=1)\n",
    "#             pooled_hidden_state = last_hidden_state.mean(dim=1)\n",
    "            \n",
    "#             hidden_states.append(pooled_hidden_state.cpu().numpy())\n",
    "            \n",
    "#             # Extract tokens for each input\n",
    "#             for batch_input_ids in input_ids:\n",
    "#                 tokens.append([tokenizer.decode(token_id) for token_id in batch_input_ids if token_id not in tokenizer.all_special_ids])\n",
    "\n",
    "#     return np.vstack(hidden_states), tokens  # Return hidden states and tokens\n",
    "\n",
    "# # Extract hidden states and tokens for the validation set\n",
    "# hidden_states, tokens = extract_hidden_states(model_classifier, dataloader)\n",
    "\n",
    "# # Apply t-SNE to reduce dimensionality to 2 components\n",
    "# tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)  # Adjust perplexity and n_iter if needed\n",
    "# tsne_result = tsne.fit_transform(hidden_states)\n",
    "\n",
    "# # Extract true labels from the validation dataset (make sure labels are numeric)\n",
    "# true_labels_tensor = torch.tensor(tokenized_dataset['labels'])\n",
    "# true_labels = true_labels_tensor.numpy()\n",
    "\n",
    "# # Prepare the token data for the plot\n",
    "# # Flatten the list of tokens so that each token corresponds to a t-SNE point\n",
    "# flat_tokens = [token for token_list in tokens for token in token_list]\n",
    "# flat_tsne_result = np.repeat(tsne_result, [len(token_list) for token_list in tokens], axis=0)\n",
    "# flat_labels = np.repeat(true_labels, [len(token_list) for token_list in tokens])\n",
    "\n",
    "# # Create a DataFrame with t-SNE results and token information\n",
    "# df_tsne = pd.DataFrame(flat_tsne_result, columns=['TSNE Component 1', 'TSNE Component 2'])\n",
    "# df_tsne['Label'] = flat_labels\n",
    "# df_tsne['Token'] = flat_tokens\n",
    "\n",
    "# # Create an interactive t-SNE plot using Plotly\n",
    "# fig = px.scatter(df_tsne, x='TSNE Component 1', y='TSNE Component 2',\n",
    "#                  color='Label',\n",
    "#                  hover_name='Token',  # Show token as hover information\n",
    "#                  title=\"t-SNE of Hidden States from XLM-RoBERTa (Tokens)\",\n",
    "#                  labels={'Label': 'Class'},\n",
    "#                  color_continuous_scale='Viridis')\n",
    "\n",
    "# # Show the figure\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Predictions and Evaluation (Specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n",
    "# from datasets import load_dataset\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Load the pretrained tokenizer and fine-tuned model\n",
    "# tokenizer = AutoTokenizer.from_pretrained('./finetuned_xlm_roberta')\n",
    "# model_classifier = AutoModelForSequenceClassification.from_pretrained('./finetuned_xlm_roberta')\n",
    "\n",
    "# # Load the test dataset\n",
    "# test_dataset = load_dataset('csv', data_files={'test': 'test_data.csv'})['test']\n",
    "\n",
    "# # Encode the labels using LabelEncoder\n",
    "# label_encoder = LabelEncoder()\n",
    "# label_encoder.fit(['iindaba', 'ezemidlalo'])  # Modify labels according to your dataset\n",
    "\n",
    "# def encode_labels(examples):\n",
    "#     examples['labels'] = label_encoder.transform(examples['label'])\n",
    "#     return examples\n",
    "\n",
    "# # Apply label encoding to the test dataset\n",
    "# test_dataset = test_dataset.map(encode_labels, batched=True)\n",
    "\n",
    "# # Tokenize the test dataset\n",
    "# def tokenize_classification(examples):\n",
    "#     return tokenizer(\n",
    "#         examples['headlines'],\n",
    "#         padding=\"max_length\",\n",
    "#         truncation=True,\n",
    "#         max_length=512\n",
    "#     )\n",
    "\n",
    "# tokenized_test_dataset = test_dataset.map(\n",
    "#     tokenize_classification,\n",
    "#     batched=True,\n",
    "#     remove_columns=['headlines', 'label']  # Remove original columns\n",
    "# )\n",
    "\n",
    "# # Initialize Trainer\n",
    "# trainer_classifier = Trainer(\n",
    "#     model=model_classifier,\n",
    "# )\n",
    "\n",
    "# # Make predictions on the tokenized test dataset\n",
    "# predictions = trainer_classifier.predict(tokenized_test_dataset)\n",
    "\n",
    "# # Extract logits and predicted class labels\n",
    "# logits = predictions.predictions\n",
    "# predicted_label_ids = np.argmax(logits, axis=-1)\n",
    "\n",
    "# # True labels (after tokenization)\n",
    "# true_labels = tokenized_test_dataset['labels']\n",
    "\n",
    "# # Generate confusion matrix\n",
    "# conf_matrix = confusion_matrix(true_labels, predicted_label_ids)\n",
    "\n",
    "# # Plot confusion matrix\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "#             xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "# plt.title(\"Confusion Matrix\")\n",
    "# plt.xlabel(\"Predicted Labels\")\n",
    "# plt.ylabel(\"True Labels\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained('FacebookAI/xlm-roberta-base')\n",
    "# base_model_classifier = AutoModelForSequenceClassification.from_pretrained('FacebookAI/xlm-roberta-base')\n",
    "\n",
    "# # Load the test dataset\n",
    "# test_dataset = load_dataset('csv', data_files={'test': 'test_data.csv'})['test']\n",
    "\n",
    "# # Encode the labels using LabelEncoder\n",
    "# label_encoder = LabelEncoder()\n",
    "# label_encoder.fit(['iindaba', 'ezemidlalo'])  # Modify labels according to your dataset\n",
    "\n",
    "# def encode_labels(examples):\n",
    "#     examples['labels'] = label_encoder.transform(examples['label'])\n",
    "#     return examples\n",
    "\n",
    "# # Apply label encoding to the test dataset\n",
    "# test_dataset = test_dataset.map(encode_labels, batched=True)\n",
    "\n",
    "# # Tokenize the test dataset\n",
    "# def tokenize_classification(examples):\n",
    "#     return tokenizer(\n",
    "#         examples['headlines'],\n",
    "#         padding=\"max_length\",\n",
    "#         truncation=True,\n",
    "#         max_length=512\n",
    "#     )\n",
    "\n",
    "# tokenized_test_dataset = test_dataset.map(\n",
    "#     tokenize_classification,\n",
    "#     batched=True,\n",
    "#     remove_columns=['headlines', 'label']  # Remove original columns\n",
    "# )\n",
    "\n",
    "# # Initialize Trainer\n",
    "# trainer_classifier = Trainer(\n",
    "#     model=base_model_classifier,\n",
    "# )\n",
    "\n",
    "# # Make predictions on the tokenized test dataset\n",
    "# predictions = trainer_classifier.predict(tokenized_test_dataset)\n",
    "\n",
    "# # Extract logits and predicted class labels\n",
    "# logits = predictions.predictions\n",
    "# predicted_label_ids = np.argmax(logits, axis=-1)\n",
    "\n",
    "# # True labels (after tokenization)\n",
    "# true_labels = tokenized_test_dataset['labels']\n",
    "\n",
    "# # Generate confusion matrix\n",
    "# conf_matrix = confusion_matrix(true_labels, predicted_label_ids)\n",
    "\n",
    "# # Plot confusion matrix\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "#             xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "# plt.title(\"Confusion Matrix\")\n",
    "# plt.xlabel(\"Predicted Labels\")\n",
    "# plt.ylabel(\"True Labels\")\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
